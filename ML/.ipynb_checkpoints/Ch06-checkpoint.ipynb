{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1071420e",
   "metadata": {},
   "source": [
    "## 비지도 학습(Unsupervised learning)과 데이터 전처리\n",
    "- 비지도 학습이란 알고 있는 출력값이나 정보 없이 학습 알고리즘을 가르쳐야 하는 모든 종류의 머신러닝을 말함  \n",
    "- 비지도 학습에서 학습 알고리즘은 입력 데이터만으로 데이터에서 지식을 추출할 수 있어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaf44d",
   "metadata": {},
   "source": [
    "## 비지도 학습의 종류\n",
    "- 두 가지 비지도 학습을 알아봄  \n",
    "- 데이터의 비지도 변환(Unsupervised transformation)과 군집(Clustering)  \n",
    "- 비지도 변환은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘  \n",
    "- 비지도 변환이 널리 사용되는 분야는 특성이 많은 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원 축소(Dimensionality Reduction)  \n",
    "- 차원 축소의 대표적 예는 시각화를 위해 데이터셋을 2차원으로 변경하는 경우  \n",
    "- 비지도 변환으로 데이터를 구성하는 단위나 성분을 찾기도 함, 많은 텍스트 문서에서 주제를 추출하는 것이 이런 예  \n",
    "- 이때 처리할 작업은 문서에서 이야기하는 주제들이 무엇인지 찾고 학습하는 것  \n",
    "- 이는 소셜 미디어에서 선거, 총기 규제, 팝스타 같은 주제로 일어나는 토론을 추적할 때 사용할 수 있음  \n",
    "- 군집 알고리즘은 데이터를 비슷한 것끼리 그룹으로 묶는 것  \n",
    "- 소셜 미디어 사이트에 사진을 업로드하는 예를 생각해 보면, 업로드한 사진을 분류하려면 같은 사람이 찍힌 사진을 같은 그룹으로 묶을 수 있음  \n",
    "- 그러나 사이트는 사진에 찍힌 사람이 누군지, 전체 사진 앨범에 얼마나 많은 사람이 있는지 알지 못함  \n",
    "- 가능한 방법은 사진에 나타난 모든 얼굴을 추출해서 비슷한 얼굴로 그룹 짓는 것임  \n",
    "- 이 얼굴들이 같은 사람의 얼굴이라면 이미지들을 구룹으로 잘 묶은 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8477f0f",
   "metadata": {},
   "source": [
    "## 비지도 학습의 도전 과제\n",
    "- 비지도 학습에서 가장 어려운 일은 알고리즘이 뭔가 유용한 것을 학습했는지 평가하는 것임  \n",
    "- 비지도 학습은 보통 레이블이 없는 데이터에 적용하기 때문에 무엇이 올바른 출력인지 모름  \n",
    "- 예를 들어 가상의 군집 알고리즘이 옆모습이 찍힌 사진과 앞모습이 찍힌 사진으로 그룹을 나눌 수 있음  \n",
    "- 이것도 사람의 얼굴 사진을 분류하는 방법이지만 우리가 원하는 방향은 아님  \n",
    "- 그러나 이 알고리즘에게 우리가 원하는 것을 알려줄 방법이 없음  \n",
    "- 아울러 비지도 학습의 결과를 평가하기 위해서는 직접 확인하는 것이 유일한 방법일 때가 많음  \n",
    "-  \n",
    "- 이런 이유로 비지도 학습 알고리즘은 데이터 과학자가 데이터를 더 잘 이해하고 싶을 때 탐색적 분석 단계에서 많이 사용하거나 지도 학습의 전처리 단계에서 사용함  \n",
    "- 비지도 학습의 결과로 새롭게 표현된 데이터를 사용해 학습하면 지도 학습의 정확도가 좋아지거나 메모리와 시간을 절약할 수 있음  \n",
    "-  \n",
    "- 먼저 몇 가지 간단한 전처리 메서드를 잠시 살펴봄(스케일 조정 메서드는 비지도 방식임)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59151598",
   "metadata": {},
   "source": [
    "## 데이터 전처리와 스케일 조정\n",
    "- SVM 같은 알고리즘은 데이터의 스케일에 매우 민감  \n",
    "- 보통 이런 알고리즘에 맞게 데이터의 특성 값을 조정함  \n",
    "- 보통 특성마다 스케일을 조정해서 데이터를 변경함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10413533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.plots.plot_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec1ecf",
   "metadata": {},
   "source": [
    "### 여러 가지 전처리 방법\n",
    "- 첫 번째 그래프는 두 개의 특성을 인위적으로 만든 이진 분류 데이터셋 임  \n",
    "- 첫 번째 특성(x 축)은 10과 15사이에 있음  \n",
    "- 두 번째 특성(y 축)은 1과 9 사이에 있음  \n",
    "- 오른쪽 네 그래프는 데이터를 기준이 되는 범위로 변환하는 네 가지 방법을 보여줌  \n",
    "- scikit-learn의 StandardScaler 는 각 특성의 평균을 0, 분산을 1로 변경하여 모든 특성이 같은 크기를 가지게 함( (x-평균)/표준편차 )  \n",
    "- 그러나 이 방법은 특성의 최솟값과 최댓값 크기를 제한하지는 않음  \n",
    "- RobustScaler는 특성들이 같은 스케일을 갖게 된다는 통계적 측면에서는 StandardScaler와 비슷함  \n",
    "- 하지만 평균과 분산 대신 중간 값(median)과 사분위 값(quartile)을 사용함  \n",
    "- 이런 방식 때문에 RobustScaler는 전체 데이터와 아주 동떨어진 데이터 포인트(outlier)에 영향을 받지 않음  \n",
    "- 반면에 MinjMaxScaler는 모든 특성이 정확하게 0과 1 사이에 위치하도록 데이터를 변경함( x - min / max - min )  \n",
    "- 2차원 데이터셋일 경우에는 모든 데이터가 x 축의 0과 1, y 축의 0과 1 사이의 사각 영역에 담기게 됨  \n",
    "- Normalizer는 매우 다른 스케일 조정 기법임  \n",
    "- 특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정, 즉 지름이 1인 원에 데이터 포인트를 투영함  \n",
    "- 이 말은 각 데이터 포인트가 다른 비율로 스케일이 조정된다는 뜻  \n",
    "- 이러한 정규화는 특성 벡터의 길이는 상관없고 데이터의 방향(또는 각도)만이 중요할 때 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ab7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44550cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda334e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# 스케일이 조정된 후 데이터셋의 속성을 출력합니다\n",
    "print(\"변환된 후 크기:\", X_train_scaled.shape)\n",
    "print(\"스케일 조정 전 특성별 최소값:\\n\", X_train.min(axis=0))\n",
    "print(\"스케일 조정 전 특성별 최대값:\\n\", X_train.max(axis=0))\n",
    "print(\"스케일 조정 후 특성별 최소값:\\n\", X_train_scaled.min(axis=0))\n",
    "print(\"스케일 조정 후 특성별 최대값:\\n\", X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d378f03",
   "metadata": {},
   "source": [
    "- 스케일을 조정한 테스트 세트의 최솟값과 최댓값이 0과 1이 아님  \n",
    "- 일부 특성은 0~1 범위를 벗어남  \n",
    "- MinMaxScaler를 포함한 모든 스케일 모델은 항상 훈련 세트와 테스트 세트에 같은 변환을 적용해야 함  \n",
    "- transform 메서드는 테스트 세트의 최솟값과 범위를 사용하지 않고, 항상 훈련 세트의 최솟값을 빼고 훈련 세트의 범위로 나눔  \n",
    "- (x<sub>test</sub> - x<sub>train_min</sub>) / (x<sub>train_max</sub> - x<sub>train_min</sub> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bfccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"테스트 세트 정확도: {:.2f}\".format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0~1 사이로 스케일 조정\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 조정된 데이터로 SVM 학습\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 스케일 조정된 테스트 세트의 정확도\n",
    "print(\"스케일 조정된 테스트 세트의 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24559f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 0, 분산 1을 갖도록 스케일 조정\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 조정된 데이터로 SVM 학습\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 스케일 조정된 테스트 세트의 정확도\n",
    "print(\"SVM 테스트 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be94be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
